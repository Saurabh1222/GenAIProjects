{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXUJspefZBVK"
      },
      "outputs": [],
      "source": [
        "# Fully upgrade all core libraries to their latest, compatible versions\n",
        "!pip install -q -U sentence-transformers transformers accelerate peft PyPDF2 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÅ Upload PDF\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "s7YQtbHhZXQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings from the transformers library\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "9L3257lPbUA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üìÑ Read PDF\n",
        "from PyPDF2 import PdfReader\n",
        "import logging\n",
        "\n",
        "def read_pdf(path): return \"\".join([p.extract_text() for p in PdfReader(path).pages])\n",
        "pdf_text = read_pdf(\"GenAI_QA_Project_Interview_Questions.pdf\")\n",
        "\n",
        "# ‚úÇÔ∏è Chunk the text\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "chunks = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_text(pdf_text)\n",
        "\n",
        "# üß† Embeddings using MiniLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = np.array([e.numpy() for e in embedder.encode(chunks, convert_to_tensor=True)])\n",
        "\n",
        "# üì¶ Store in FAISS index\n",
        "import faiss\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# üîç Retrieve top k chunks\n",
        "def retrieve_chunks(query, k=3):\n",
        "    q_emb = embedder.encode([query])[0]\n",
        "    D, I = index.search(np.array([q_emb]), k)\n",
        "    return \" \".join([chunks[i] for i in I[0]])\n",
        "\n",
        "# üí¨ Load the Flan-T5 model for answering\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "import torch\n",
        "# Use a smaller, faster model for quick inference\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "tok = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "# üîÅ Ask question\n",
        "from transformers import pipeline\n",
        "rag_pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tok)\n",
        "\n",
        "question = \"What are LLMs\"\n",
        "\n",
        "# Retrieve chunks and then truncate the context to fit the model's limit (512 tokens for T5)\n",
        "context = retrieve_chunks(question)\n",
        "# Correctly truncate the context to the model's max length and suppress the warning\n",
        "encoded_context = tok.encode(context, max_length=512, truncation=True, return_tensors=\"pt\")\n",
        "truncated_context = tok.decode(encoded_context[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "prompt = f\"Use the following context to answer the question:\\n\\nContext: {truncated_context}\\n\\nQuestion: {question}\"\n",
        "response = rag_pipe(prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "\n",
        "print(\"üìå Answer:\\n\", response.split(\"[/INST]\")[-1].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZDZLsNu5UuH",
        "outputId": "90747337-9064-4dbb-d485-e8448ce226a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìå Answer:\n",
            " LangChain is a framework to build LLM apps. We used it to handle chaining of components like loaders, chunkers, embeddings, and models.\n"
          ]
        }
      ]
    }
  ]
}